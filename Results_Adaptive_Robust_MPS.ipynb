{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b11f2f",
   "metadata": {},
   "source": [
    "## Results of Adaptive Robust Model Predictive Shielding\n",
    "This jupyter notebook illustrates the workflow to produce the results presented in the work \"Safe Reinforcement Learning via Adaptive Robust Model Predictive Shielding\". The structure of the jupyter notebook is oriented on the paper structure and divided into the different subsections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define directory to store results\n",
    "save_dir = os.path.join(os.getcwd(), 'results')\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33442cf2",
   "metadata": {},
   "source": [
    "## 4.2. RL training setup + 4.3. Penalty term for reward shaping\n",
    "In this section the influence of different reward shaping approaches on standard RL policies is investigated. For the presented evaluations for three different pentalty weights 20 RL models each have been trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15acdbf1",
   "metadata": {},
   "source": [
    "#### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26393d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_policy.RL_training import RL_training \n",
    "# penalty term for reward shaping\n",
    "lambda_weight = 5        # lambda_weight in [2.5, 5, 7.5]*1e3\n",
    "max_sp = 0.2\n",
    "save_dir_RL_base_models = rf'{save_dir}\\RL_models_base\\penalty_{lambda_weight}'\n",
    "\n",
    "# define RL training setting\n",
    "RL_settings_base = {\n",
    "    'env_type': 'CSTR_base',\n",
    "    'timesteps': int(1.5e6),\n",
    "    'gamma' : 0.999,\n",
    "    'verbose' : 1,\n",
    "    'algorithm' : 'TD3',\n",
    "    'action_noise_sigma' : 1e-2,\n",
    "    'batch_size' : 512,\n",
    "    'learning_rate' : 1e-5,\n",
    "    'eval_freq' : 2000,\n",
    "    'action_noise_type': 'OU',\n",
    "    'tau': 0.005,\n",
    "    'scheduling' : 'exponential',        # None or 'linear' or 'exponential'\n",
    "    'net': dict(pi=[400, 300], qf=[400,300]),\n",
    "    'size_of_replay_buffer' : int(1.5e6),\n",
    "    'policy_delay' : 2,\n",
    "    'save_dir': save_dir_RL_base_models,\n",
    "    'penalty_weight': lambda_weight*1e3,\n",
    "    'max_sp': max_sp,\n",
    "    'seed': 48, \n",
    "    'add_info' : 'Additionaly informaion about training setting'\n",
    "    }  \n",
    "\n",
    "# train RL agent with defined settings for different penalty weights\n",
    "num_models = 20\n",
    "\n",
    "# we recommend parallelizing the training on the CPU for faster results\n",
    "for i in range(num_models):\n",
    "    RL_dict = {**RL_settings_base,\n",
    "                'seed': 48 + i}\n",
    "    RL_training(RL_dict)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a4c4a",
   "metadata": {},
   "source": [
    "#### Evaluation - only RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e29fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only RL evaluation of trained policies without shielding\n",
    "from policy_deployment.only_RL import eval_ensemble_agents\n",
    "\n",
    "only_RL_base_results = eval_ensemble_agents(dir = save_dir_RL_base_models, num_models=num_models, num_eval = 100, sp = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a878c",
   "metadata": {},
   "source": [
    "## Section 5.1. Backup policy construction\n",
    "In this section the development of an approximate multi-stage MPC as a backup policy for model predicitve shielding is presented. The construction starts with trajectory-based sampling of the training data. Next, the training data is preprocessed and finally, several neural networks are trained in a small grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f267c0a",
   "metadata": {},
   "source": [
    "#### Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b736ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backup_policy.data_sampling import trajectory_sampling\n",
    "from RL_policy.RL_environments import CSTR_sp\n",
    "\n",
    "save_dir_backup_policy = rf'{save_dir}\\backup_policy'\n",
    "\n",
    "# environemnt for sampling of initial states\n",
    "env = CSTR_sp()\n",
    "\n",
    "# define sampling parameters\n",
    "max_sp =0.2\n",
    "alpha_var = beta_var =  np.array([1 - max_sp, 1 + max_sp])\n",
    "\n",
    "# trajectory based sampling\n",
    "trajectory_sampling(env = env, num_traj = 1000, len_traj = 100, save_dir = save_dir_backup_policy, alpha_var = alpha_var, beta_var = beta_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43373217",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backup_policy.data_sampling import drop_infeasibles, drop_duplicates, scale_data\n",
    "from RL_policy.RL_environments import CSTR_sp\n",
    "\n",
    "\n",
    "# load raw data\n",
    "x_data_raw = pd.read_csv(os.path.join(save_dir_backup_policy, 'RMPC_raw_x_data.csv'))\n",
    "y_data_raw = pd.read_csv(os.path.join(save_dir_backup_policy, 'RMPC_raw_y_data.csv'))\n",
    "feas = pd.read_csv(os.path.join(save_dir_backup_policy, 'feas_check_raw_data.csv'))\n",
    "\n",
    "# drop infeasible data points\n",
    "x_drop, y_drop = drop_infeasibles(x_data_raw, y_data_raw, feas)\n",
    "\n",
    "# drop duplicate data points\n",
    "x_red, y_red = drop_duplicates(x_drop, y_drop, n_dec=4)\n",
    "\n",
    "# scale data for training according to scaling of RL environment\n",
    "env = CSTR_sp()\n",
    "x_scaled, y_scaled = scale_data(env, x_red, y_red)\n",
    "\n",
    "# save processed data\n",
    "\n",
    "with open(os.path.join(save_dir_backup_policy, 'RMPC_x_data_red_scaled.pkl'), 'wb') as f:\n",
    "    pickle.dump(x_scaled, f)\n",
    "\n",
    "with open(os.path.join(save_dir_backup_policy, 'RMPC_y_data_red_scaled.pkl'), 'wb') as f:\n",
    "    pickle.dump(y_scaled, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b888b8",
   "metadata": {},
   "source": [
    "#### Grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65bf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backup_policy.neural_network_training import train_ARMPC, hyperparameter_list, eval_on_test_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "save_dir_grid = rf'{save_dir_backup_policy}\\grid_search'\n",
    "\n",
    "# load data and split into training and testing set\n",
    "with open(os.path.join(save_dir_backup_policy, 'RMPC_x_data_red_scaled.pkl'), 'rb') as f:\n",
    "    x_data = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(save_dir_backup_policy, 'RMPC_y_data_red_scaled.pkl'), 'rb') as f:\n",
    "    y_data = pickle.load(f)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=48)\n",
    "\n",
    "with open(os.path.join(save_dir, 'X_train.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_train, f) \n",
    "    f.close()\n",
    "\n",
    "with open(os.path.join(save_dir, 'X_test.pkl'), 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "    f.close()\n",
    "\n",
    "with open(os.path.join(save_dir, 'Y_train.pkl'), 'wb') as f:\n",
    "    pickle.dump(Y_train, f)\n",
    "    f.close()\n",
    "\n",
    "with open(os.path.join(save_dir, 'Y_test.pkl'), 'wb') as f:\n",
    "    pickle.dump(Y_test, f)\n",
    "    f.close()\n",
    "\n",
    "# hyperparameters for grid search\n",
    "hp_list = hyperparameter_list(X_train, Y_train, save_dir_grid)\n",
    "\n",
    "for hp in hp_list:\n",
    "    loss, val_loss = train_ARMPC(X_train, Y_train, hp)\n",
    "    test_loss = eval_on_test_data(X_test, Y_test, rf'{save_dir_grid}\\grid{hp[\"number\"]}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add449fd",
   "metadata": {},
   "source": [
    "#### Probabilistic validation of backup policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eaac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backup_policy.auxiliary_functions_NN import compute_N_prob\n",
    "from policy_deployment.only_RL import closed_loop_evaluation\n",
    "\n",
    "# backup policy for probabilistic validation\n",
    "backup_dir = save_dir_grid + r'\\grid23'\n",
    "\n",
    "# Define parameters for sampling discarding probabilistic validation\n",
    "epsilon = 0.005  # probability of violation\n",
    "r = 1            # number of discarded samples\n",
    "delta = 10**-6   # confidence\n",
    "\n",
    "N_prob = compute_N_prob(epsilon, r, delta)      # result is rounded up inside the function\n",
    "\n",
    "# evaluate N_prob trajectories for probabilistic validation\n",
    "backup_validation = closed_loop_evaluation(dir = backup_dir, num_eval = N_prob, sp = False, RL_agent=False, feas_IC=True) \n",
    "if np.count_nonzero(backup_validation['CV_all'].to_numpy()) <= r:\n",
    "    print('Successful probabilistic validation!')\n",
    "else:\n",
    "    print('Probabilistic validation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ee276",
   "metadata": {},
   "source": [
    "## Section 5.2. Robust model predicitve shielding\n",
    "In this section the constructed approximate multi-stage mpc is applied as a backup policy for model predicitve shielding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e11e5",
   "metadata": {},
   "source": [
    "#### Evaluation - Robust MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_deployment.MPS import shielded_deployment\n",
    "from RL_policy.RL_environments import CSTR_sp\n",
    "\n",
    "max_sp = 0.2\n",
    "\n",
    "# tuning parameters\n",
    "n_horizon = 20\n",
    "num_MC = None  # number of Monte Carlo simulations for robust rollouts, if set to None, min-max approach is applied and four rollouts are performed\n",
    "\n",
    "backup_dir = save_dir_grid + r'\\grid23'\n",
    "agent_dir = save_dir_RL_base_models + r'\\model_0'\n",
    "\n",
    "env = CSTR_sp(seed = 123)\n",
    "sampling = 'uniform'\n",
    "\n",
    "# exemplarily deployment with robust shielding for one RL policy\n",
    "shielded_deployment_instance = shielded_deployment(agent_dir=agent_dir, backup_dir=backup_dir, env=env, max_sp=max_sp, adaptive=False, n_horizon=n_horizon)\n",
    "shielded_deployment_instance.shielded_closed_loop(num_eval=1, num_MC = num_MC, sampling=sampling)\n",
    "\n",
    "# results are stored in a dataframe in attribute results_df\n",
    "results = shielded_deployment_instance.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88030a4b",
   "metadata": {},
   "source": [
    "## Section 5.3. RL training for adaptive shielding\n",
    "In this section RL policies are trained for the augmented state space with the safety parameter $\\sigma$. The training hyperparameters are set identically to those used in the training process of RL models without safety parameters. Again, 20 RL policies are trained for each reward. Afterwards, the monotonicity of the safety parameter is investigated to determine the policies considered for further evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5a795",
   "metadata": {},
   "source": [
    "#### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e60d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_policy.RL_training import RL_training   \n",
    "\n",
    "lambda_weight = 5        # lambda_weight in [2.5, 5, 7.5]*1e3\n",
    "\n",
    "save_dir_RL_sp_models = rf'{save_dir}\\RL_models_sp\\penalty_{lambda_weight}'\n",
    "\n",
    "RL_settings_sp = RL_settings_base.copy()\n",
    "RL_settings_sp.update({'fix_p_unc':True,\n",
    "                       'env_type': 'CSTR_sp',\n",
    "                       'lambda_weight': lambda_weight*1e3,\n",
    "                       'save_dir': save_dir_RL_sp_models})\n",
    "\n",
    "# train RL agent with defined settings for different penalty weights\n",
    "num_models = 20\n",
    "\n",
    "for i in range(num_models):\n",
    "    RL_dict = {**RL_settings_sp,\n",
    "                'seed': 48 + i}\n",
    "    RL_training(RL_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9884d",
   "metadata": {},
   "source": [
    "#### Monotonicity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_deployment.only_RL import monotonicity_check\n",
    "\n",
    "# results are stored in agent_dir\n",
    "monotonicity_check(agent_dir=save_dir_RL_sp_models, num_models=num_models, episode_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74b56b",
   "metadata": {},
   "source": [
    "#### Evaluation - only RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy_deployment.only_RL import eval_ensemble_agents\n",
    "\n",
    "# only RL evaluation of trained policies without shielding, results are stored in only_RL_sp_results\n",
    "only_RL_base_results = eval_ensemble_agents(dir = save_dir_RL_sp_models, num_models=num_models, num_eval = 100, sp = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6fcbd",
   "metadata": {},
   "source": [
    "## Section 5.4. Adaptive robust model predictive shielding\n",
    "In the following, the shielded deployment of RL policies augmented with the safety parameter is presented. We exemplarily show the evaluation of one model for the robust MPS, adaptive MPS, and adaptive robust MPS settings. Note that the same backup policy as derived in Section 5.1 and applied in Section 5.2 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_policy.RL_environments import CSTR_sp\n",
    "from policy_deployment.MPS import shielded_deployment\n",
    "\n",
    "# define backup policy and tuning parameters for all evaluations\n",
    "backup_dir = save_dir_grid + r'\\grid23'\n",
    "\n",
    "n_horizon = 20\n",
    "num_MC = None  # number of Monte Carlo simulations for robust rollouts, if set to None, min-max approach is applied and four rollouts are performed\n",
    "\n",
    "# tuning parameters for adaptation of safety parameter\n",
    "step_sp = 0.025\n",
    "max_sp = 0.2\n",
    "\n",
    "# environment\n",
    "env = CSTR_sp(seed = 123)\n",
    "sampling = 'uniform'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3700e9",
   "metadata": {},
   "source": [
    "#### Evaluation - robust MPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dir = save_dir_RL_sp_models + r'\\model_0'\n",
    "\n",
    "# exemplarily deployment with robust shielding for one RL policy\n",
    "shielded_deployment_instance = shielded_deployment(agent_dir=agent_dir, backup_dir=backup_dir, env=env, max_sp=max_sp, sp = True, adaptive=False, n_horizon=n_horizon)\n",
    "shielded_deployment_instance.shielded_closed_loop(num_eval=100, num_MC = num_MC, sampling=sampling)\n",
    "\n",
    "# results are stored in a dataframe in attribute results_df\n",
    "results = shielded_deployment_instance.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c65df9",
   "metadata": {},
   "source": [
    "#### Evaluation - adaptive MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dir = save_dir_RL_sp_models + r'\\model_0'\n",
    "\n",
    "# exemplarily deployment with adapitve shielding for one RL policy\n",
    "shielded_deployment_instance = shielded_deployment(agent_dir=agent_dir, env=env, max_sp=max_sp, sp = True, adaptive=True, n_horizon=n_horizon)\n",
    "shielded_deployment_instance.shielded_closed_loop(num_eval=100, num_MC = num_MC, sampling=sampling)\n",
    "\n",
    "# results are stored in a dataframe in attribute results_df\n",
    "results = shielded_deployment_instance.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d8ad1",
   "metadata": {},
   "source": [
    "#### Evaluation - adaptive robust MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dir = save_dir_RL_sp_models + r'\\model_0'\n",
    "\n",
    "# exemplarily deployment with robust shielding for one RL policy\n",
    "shielded_deployment_instance = shielded_deployment(agent_dir=agent_dir, backup_dir=backup_dir, env=env, max_sp=max_sp, sp = True, adaptive=True, n_horizon=n_horizon)\n",
    "shielded_deployment_instance.shielded_closed_loop(num_eval=100, num_MC=num_MC, sampling=sampling)\n",
    "\n",
    "# results are stored in a dataframe in attribute results_df\n",
    "results = shielded_deployment_instance.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90edd89",
   "metadata": {},
   "source": [
    "#### $\\beta$ distribution sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust sampling method and repeat evaluations with shield and only RL evaluations\n",
    "sampling = 'beta'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b6561",
   "metadata": {},
   "source": [
    "## Section 5.5. Comparison with alternative backup policy\n",
    "In this section, the evaluations using multi-stage MPC as an alternative backup policy are presented. Again, only the exemplary evaluation of one model is presented. The evaluation is to be performed for the base RL policies without safety parameter for the robust MPS setting, and for RL policies with safety parameter for the robust MPS and adaptive robust MPS settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_policy.RL_environments import CSTR_sp\n",
    "from policy_deployment.MPS import shielded_deployment_multi_stage_mpc\n",
    "\n",
    "n_horizon = 20\n",
    "\n",
    "# tuning parameters for adaptation of safety parameter\n",
    "step_sp = 0.025\n",
    "max_sp = 0.2\n",
    "\n",
    "# system environment\n",
    "env = CSTR_sp(seed = 123) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63944a4e",
   "metadata": {},
   "source": [
    "#### Evaluation - robust MPS with multi-stage MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cecc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dir = save_dir_RL_sp_models + r'\\model_0'\n",
    "\n",
    "# exemplarily deployment with robust shielding for one RL policy\n",
    "shielded_deployment_instance = shielded_deployment_multi_stage_mpc(agent_dir=agent_dir, env=env, max_sp=max_sp, sp = True, adaptive=False, n_horizon=n_horizon)\n",
    "shielded_deployment_instance.shielded_closed_loop(num_eval=100)\n",
    "\n",
    "# results are stored in a dataframe in attribute results_df\n",
    "results = shielded_deployment_instance.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2b888",
   "metadata": {},
   "source": [
    "#### Evaluation - adaptive robust MPS with multi-stage MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf03506",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dir = save_dir_RL_sp_models + r'\\model_0'\n",
    "\n",
    "# exemplarily deployment with robust shielding for one RL policy\n",
    "shielded_deployment_instance = shielded_deployment_multi_stage_mpc(agent_dir=agent_dir, env=env, max_sp=max_sp, sp = True, adaptive=True, n_horizon=n_horizon)\n",
    "shielded_deployment_instance.shielded_closed_loop(num_eval=100)\n",
    "\n",
    "# results are stored in a dataframe in attribute results_df\n",
    "results = shielded_deployment_instance.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc98b40",
   "metadata": {},
   "source": [
    "## Section 5.6. Generalizability of safety parameter\n",
    "We refrain from showing the entire procedure to demonstrate the genaralizability of the safety parameter and limit ourselves to a brief description: The above reported procedures to describe the RL models (with and without safety parameter) and the construction of the backup policy are repeated for a new maximum value of the safety parameter, which is set to $\\sigma_{\\text{max}}=0.15$ for the RL environments and neural network training data sampling. The evaluations are performed for $\\sigma_{\\text{max}}=0.15$ and $\\sigma_{\\text{max}}=0.2$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455dba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat RL training and backup policy construction for narrowed safety parameter range and repeat evaluation for previous safety parameter range\n",
    "max_sp = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a70315",
   "metadata": {},
   "source": [
    "## Section 5.7. Investigation of tuning parameters\n",
    "We refrain from showing the entire investigation of tuning parameters. To reproduce our presented results, the above described shielding evaluations are to be repeated for the different values of $N_{\\text{roll}}$, $\\alpha_\\sigma$ and $N$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust tuning parameters and repeat evaluation described above\n",
    "n_horizon = 20            # [5, 10, 20]\n",
    "num_MC = None             # [10, 100, 1000, None]\n",
    "step_sp = 0.05            # [0.025, 0.05, 0.1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLHF_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
